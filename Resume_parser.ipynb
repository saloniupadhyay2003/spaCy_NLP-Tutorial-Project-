{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORQEWBn9Kxfuylfuaala0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saloniupadhyay2003/spaCy_tutorial/blob/main/Resume_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbDBcyK0508c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RESUME PARSER USING SPACY"
      ],
      "metadata": {
        "id": "XYU2HaAya-2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data:\n",
        "\n",
        "\n",
        "*   https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s_PFNpVX8ikr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installations"
      ],
      "metadata": {
        "id": "FO-69JBYbFOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy_transformers\n",
        "!pip install -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVtuFlTW8lTD",
        "outputId": "de7188aa-eb43-4f15-e10b-a44d5ec5be41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy_transformers in /usr/local/lib/python3.10/dist-packages (1.2.5)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (1.22.4)\n",
            "Requirement already satisfied: transformers<4.31.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (4.30.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (2.4.7)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (0.9.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.1.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.10.11)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy_transformers) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (0.16.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (0.3.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.31.0,>=3.4.0->spacy_transformers) (2023.6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy_transformers) (1.3.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.11)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "fqvoV2IE9OTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3JHROElGOdN",
        "outputId": "02cc7a33-96e5-489a-d811-dbbb54c9cf1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 25 18:40:57 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQQufsxaGRXe",
        "outputId": "7cb666ba-c41d-49c0-9cce-37636cdb111d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CV-Parsing-using-Spacy-3' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "N-kdKUSkGk20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING DATA"
      ],
      "metadata": {
        "id": "yJVoyNstbKAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data = json.load(open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json','r'))"
      ],
      "metadata": {
        "id": "HNnu9F83Gv9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cv_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhhLxAzRHAue",
        "outputId": "7ef727f3-73a1-414a-c810-138c9590619e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/CV-Parsing-using-Spacy-3/data/training/base_config.cfg /content/CV-Parsing-using-Spacy-3/data/training/config.cfg\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJifFmXDHDrN",
        "outputId": "34404b39-db94-47e0-b154-dbd41fd88d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-25 19:38:25.023120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/CV-Parsing-using-Spacy-3/data/training/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5UQ49WXI2Vu",
        "outputId": "296953f9-f435-4dc2-f004-601decb96b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending Solutions INC 4 Month • Salesforce Developer Oracle 5 Years 2 Month • Core Java Developer Languages Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations & Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -  Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore, Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -  November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of Technology -  Tamil Nadu  September 2008 to June 2012  https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN   SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice  Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware: Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x  https://www.linkedin.com/in/govardhana-k-61024944/',\n",
              " {'entities': [[1749, 1755, 'Companies worked at'],\n",
              "   [1696, 1702, 'Companies worked at'],\n",
              "   [1417, 1423, 'Companies worked at'],\n",
              "   [1356, 1793, 'Skills'],\n",
              "   [1209, 1215, 'Companies worked at'],\n",
              "   [1136, 1247, 'Skills'],\n",
              "   [928, 932, 'Graduation Year'],\n",
              "   [858, 889, 'College Name'],\n",
              "   [821, 856, 'Degree'],\n",
              "   [787, 791, 'Graduation Year'],\n",
              "   [744, 750, 'Companies worked at'],\n",
              "   [722, 742, 'Designation'],\n",
              "   [658, 664, 'Companies worked at'],\n",
              "   [640, 656, 'Designation'],\n",
              "   [574, 580, 'Companies worked at'],\n",
              "   [555, 572, 'Designation'],\n",
              "   [470, 493, 'Companies worked at'],\n",
              "   [444, 468, 'Designation'],\n",
              "   [308, 314, 'Companies worked at'],\n",
              "   [234, 240, 'Companies worked at'],\n",
              "   [175, 198, 'Companies worked at'],\n",
              "   [93, 136, 'Email Address'],\n",
              "   [39, 48, 'Location'],\n",
              "   [13, 37, 'Designation'],\n",
              "   [0, 12, 'Name']]}]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spacy_doc(file, data):\n",
        "  nlp = spacy.blank('en')\n",
        "  db = DocBin()\n",
        "\n",
        "  for text, annot in tqdm(data):\n",
        "    doc = nlp.make_doc(text)\n",
        "    annot = annot['entities']\n",
        "\n",
        "    ents = []\n",
        "    entity_indices = []\n",
        "\n",
        "    for start, end, label in annot:\n",
        "      skip_entity = False\n",
        "      for idx in range(start, end):\n",
        "        if idx in entity_indices:\n",
        "          skip_entity=True\n",
        "          break\n",
        "      if skip_entity==True:\n",
        "        continue\n",
        "\n",
        "      entity_indices = entity_indices + list(range(start, end))\n",
        "\n",
        "      try:\n",
        "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      if span is None:\n",
        "        err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
        "        file.write(err_data)\n",
        "\n",
        "      else:\n",
        "        ents.append(span)\n",
        "\n",
        "    try:\n",
        "      doc.ents = ents\n",
        "      db.add(doc)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return db\n"
      ],
      "metadata": {
        "id": "m-36B9-gL-lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING MODEL"
      ],
      "metadata": {
        "id": "z6otCJQ7bOQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cv_data, test_size = 0.3)"
      ],
      "metadata": {
        "id": "Veqlaj45SUH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEfA_vFSS7Ff",
        "outputId": "624fb5a9-3159-435d-b730-9f8c852562d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('error.txt', 'w')\n",
        "\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk('test_data.spacy')\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DsCFfxYTDo3",
        "outputId": "92b144da-a8b5-4323-b144-4abe4f1c4ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [00:01<00:00, 92.33it/s] \n",
            "100%|██████████| 60/60 [00:00<00:00, 82.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNdkb8hXTNrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(db.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu01vcrGTdft",
        "outputId": "1e2392e8-e150-4bdc-d0ba-f022d77aac19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PatbFBWVVOR",
        "outputId": "2a6d7269-1bb9-40e9-da93-36d230bdbd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-25 19:39:52.884876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-07-25 19:40:01,968] [INFO] Set up nlp object from config\n",
            "[2023-07-25 19:40:01,985] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2023-07-25 19:40:01,989] [INFO] Created vocabulary\n",
            "[2023-07-25 19:40:01,989] [INFO] Finished initializing nlp object\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2023-07-25 19:40:17,182] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        5091.16   1331.05    0.00    0.00    0.00    0.00\n",
            "  4     200      315649.11  92409.70   12.92   44.12    7.57    0.13\n",
            "  8     400      121610.58  37108.98   42.17   43.14   41.24    0.42\n",
            " 12     600       28814.58  30690.62   51.09   57.80   45.78    0.51\n",
            " 16     800       28774.51  28099.95   50.40   59.18   43.88    0.50\n",
            " 20    1000       35923.54  27715.54   53.44   55.01   51.95    0.53\n",
            " 24    1200       17549.81  26554.76   49.59   54.45   45.52    0.50\n",
            " 28    1400        2236.28  24804.27   54.55   55.80   53.34    0.55\n",
            " 32    1600        9918.16  24277.08   56.68   57.80   55.61    0.57\n",
            " 36    1800       14533.36  24090.30   56.21   55.93   56.49    0.56\n",
            " 40    2000         560.57  23262.64   54.15   54.85   53.47    0.54\n",
            " 44    2200         905.28  23122.97   53.55   50.39   57.12    0.54\n",
            " 48    2400       21055.27  23377.81   55.09   49.06   62.80    0.55\n",
            " 53    2600         483.93  21934.28   54.77   48.63   62.67    0.55\n",
            " 57    2800         278.26  21544.63   54.49   48.57   62.04    0.54\n",
            " 61    3000       14749.07  21180.97   56.05   52.48   60.15    0.56\n",
            " 65    3200         251.82  20559.72   55.83   49.32   64.31    0.56\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model test"
      ],
      "metadata": {
        "id": "-SOUNIflxlBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('/content/output/model-best')"
      ],
      "metadata": {
        "id": "FYtmhAwYYnAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('my name is Laxmi Kant , I worked at Amazon. I have 2 years of experience')\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, \"   ->>>>>>>> \", ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wZIXpEJxygQ",
        "outputId": "230187df-639a-4046-eb57-419439edd77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laxmi Kant    ->>>>>>>>  Name\n",
            "Amazon    ->>>>>>>>  Companies worked at\n",
            "2 years    ->>>>>>>>  Years of Experience\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLmXMpY2yJyr",
        "outputId": "3477143e-e1ea-4e18-977b-af84bfcab1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.22.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.22.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, fitz"
      ],
      "metadata": {
        "id": "dX5NoIdHyf59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fname = '/content/CV-Parsing-using-Spacy-3/data/test/Smith Resume.pdf'"
      ],
      "metadata": {
        "id": "mRCxhV1byk4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = fitz.open(fname)"
      ],
      "metadata": {
        "id": "eIpIvQ_QypKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtnmFILDyrXu",
        "outputId": "7a5e3790-33bb-428a-feed-335ea2689331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document('/content/CV-Parsing-using-Spacy-3/data/test/Smith Resume.pdf')"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \"\n",
        "for page in doc:\n",
        "  text = text + str(page.get_text())"
      ],
      "metadata": {
        "id": "3I1oEIOFytug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "8t323cHOy1zB",
        "outputId": "0f7eb458-dbd8-47b3-e221-3c97e2142bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Michael Smith \\nBI / Big Data/ Azure \\nManchester, UK- Email me on Indeed: indeed.com/r/falicent/140749dace5dc26f \\n \\n10+ years of Experience in Designing, Development, Administration, Analysis, \\nManagement \\ninthe \\nBusiness \\nIntelligence \\nData \\nwarehousing, \\nClient \\nServer \\nTechnologies, Web-based Applications, cloud solutions and Databases. \\nData warehouse: Data analysis, star/ snow flake schema data modeling and design \\nspecific todata warehousing and business intelligence environment. \\nDatabase: Experience in database designing, scalability, back-up and recovery, \\nwriting andoptimizing SQL code and Stored Procedures, creating functions, views, \\ntriggers and indexes.  \\nCloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL \\nAzure, StreamAnalytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure \\ndata lake analytics(U-SQL). \\nBig Data: Worked Azure data lake store/analytics for big data processing and Azure \\ndata factoryto schedule U-SQL jobs. Designed and developed end to end big data \\nsolution for data insights.  \\n \\nWilling to relocate: Anywhere \\nWORK EXPERIENCESoftware Engineer \\nMicrosoft - Manchester, UK. \\nDecember 2015 to Present \\n1. Microsoft Rewards Live dashboards: \\nDescription: - Microsoft rewards is loyalty program that rewards Users for \\nbrowsing and shopping online. Microsoft Rewards members can earn points when \\nsearching with Bing, browsing with Microsoft Edge and making purchases at the \\nXbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up \\nbonus points for taking daily quizzes and tours on the Microsoft rewards website. \\nRewards live dashboards gives a live picture of usage world-wide and by markets \\nlike US, Canada, Australia, new user registration count, top/bottom performing \\nrewards offers, orders stats and weekly trends of user activities, orders and new \\nuser registrations. the PBI tiles gets refreshed in different frequencies starting \\nfrom 5 seconds to 30 minutes. \\nTechnology/Tools used \\nEvent hub, stream analytics and Power BI. \\nResponsibilities \\nCreated stream analytics jobs to process event hub data \\nCreated Power BI live dashboard to show live usage traffic, weekly trends, cards, \\ncharts to showtop/bottom 10 offers and usage metrics. \\n2. Microsoft Rewards Data Insights: \\nDescription: - Microsoft rewards is loyalty program that rewards Users for \\nbrowsing and shopping online. Microsoft Rewards members can earn points when \\nsearching with Bing, browsing with Microsoft Edge and making purchases at the \\nXbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up \\nbonus points for taking daily quizzes and tours on the Microsoft rewards website. \\nRewards data insights is data analytics and reporting platform, processes 20 \\nmillion users daily activities and redemption across different markets like US, \\nCanada, Australia. \\nTechnology/Tools used \\nCosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power BI. \\nResponsibilities \\nCreated big data scripts in cosmos \\nC# data extractors, processors and reducers for data transformation \\nPower BI dashboards \\n3. End to end tracking Tool: \\nDescription: - This is real-time Tracking tool to track different business \\ntransactions like order, order response, functional acknowledgement, invoice \\nflowing inside ICOE. It gives flexibility to customers to track their transactions \\nand appropriate error information in-case of any failure. Based on resource based \\naccess control the tool gives flexibility to end user to perform different actions \\nlike view transactions, search based on different filter criteria and view and \\ndownload actual message payload. End to end tracking tool stitches all the \\nbusiness transaction like order to cash flow and connects different hops inside \\nICOE like gateway, routing server, Processing server. It also connects different \\nsystems like ICOE, partner end point and SAP. \\nTechnology/Tools used \\nAzure Document db, Azure web job and Web APP, RBAC, Angular JS. \\nResponsibilities \\nDocument dB stored procedures. \\nWeb job to process event hub data and populate Document db• Web App API. \\nStream analytics job to transform data \\nPower BI reports \\n4. Biztrack Tracking Tool: \\nDescription: - This is real-time Tracking tool to track different business \\ntransactions like order, order response, functional acknowledgement, invoice \\nflowing inside ICOE. It gives flexibility to customers to track their transactions \\nand appropriate error information in-case of any failure. Based on resource based \\naccess control the tool gives flexibility to end user to perform different actions \\nlike view transactions, search based on different filter criteria and view and \\ndownload actual message payload. \\nTechnology/Tools used \\nSQL server 2014, SSIS, .net API, Angular JS. \\nResponsibilities \\nETL solution to transform business transactions data stored in Biztalk tables. \\nSQL azure tables, stored procedures, User defined functions. \\nPerformance tuning. \\nWeb API enhancements. \\n \\nEDUCATION \\nThe University of Manchester - UK \\n2007 \\n \\nSKILLS \\nproblem solving (Less than 1 year), project lifecycle (Less than 1 year), project \\nmanager (Less than 1 year), technical assistance. (Less than 1 year) \\nADDITIONAL INFORMATION \\nProfessional Skills \\nExcellent analytical, problem solving, communication, knowledge transfer and \\ninterpersonalskills with ability to interact with individuals at all the levels \\nQuick learner and maintains cordial relationship with project manager and team \\nmembers andgood performer both in team and independent job environments \\nPositive attitude towards superiors &amp; peers \\nSupervised junior developers throughout project lifecycle and provided technical \\nassistance. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.strip()"
      ],
      "metadata": {
        "id": "Si9JUD7Sy2ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \" \".join(text.split())"
      ],
      "metadata": {
        "id": "BD8KAYkgzoN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "DBQmxXwSzpeN",
        "outputId": "c693d089-9956-4440-df1f-d9442a405279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Michael Smith BI / Big Data/ Azure Manchester, UK- Email me on Indeed: indeed.com/r/falicent/140749dace5dc26f 10+ years of Experience in Designing, Development, Administration, Analysis, Management inthe Business Intelligence Data warehousing, Client Server Technologies, Web-based Applications, cloud solutions and Databases. Data warehouse: Data analysis, star/ snow flake schema data modeling and design specific todata warehousing and business intelligence environment. Database: Experience in database designing, scalability, back-up and recovery, writing andoptimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, StreamAnalytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake analytics(U-SQL). Big Data: Worked Azure data lake store/analytics for big data processing and Azure data factoryto schedule U-SQL jobs. Designed and developed end to end big data solution for data insights. Willing to relocate: Anywhere WORK EXPERIENCESoftware Engineer Microsoft - Manchester, UK. December 2015 to Present 1. Microsoft Rewards Live dashboards: Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping online. Microsoft Rewards members can earn points when searching with Bing, browsing with Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft rewards website. Rewards live dashboards gives a live picture of usage world-wide and by markets like US, Canada, Australia, new user registration count, top/bottom performing rewards offers, orders stats and weekly trends of user activities, orders and new user registrations. the PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. Technology/Tools used Event hub, stream analytics and Power BI. Responsibilities Created stream analytics jobs to process event hub data Created Power BI live dashboard to show live usage traffic, weekly trends, cards, charts to showtop/bottom 10 offers and usage metrics. 2. Microsoft Rewards Data Insights: Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping online. Microsoft Rewards members can earn points when searching with Bing, browsing with Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft rewards website. Rewards data insights is data analytics and reporting platform, processes 20 million users daily activities and redemption across different markets like US, Canada, Australia. Technology/Tools used Cosmos (Microsoft big-data platform), c#, X-flow job monitoring, Power BI. Responsibilities Created big data scripts in cosmos C# data extractors, processors and reducers for data transformation Power BI dashboards 3. End to end tracking Tool: Description: - This is real-time Tracking tool to track different business transactions like order, order response, functional acknowledgement, invoice flowing inside ICOE. It gives flexibility to customers to track their transactions and appropriate error information in-case of any failure. Based on resource based access control the tool gives flexibility to end user to perform different actions like view transactions, search based on different filter criteria and view and download actual message payload. End to end tracking tool stitches all the business transaction like order to cash flow and connects different hops inside ICOE like gateway, routing server, Processing server. It also connects different systems like ICOE, partner end point and SAP. Technology/Tools used Azure Document db, Azure web job and Web APP, RBAC, Angular JS. Responsibilities Document dB stored procedures. Web job to process event hub data and populate Document db• Web App API. Stream analytics job to transform data Power BI reports 4. Biztrack Tracking Tool: Description: - This is real-time Tracking tool to track different business transactions like order, order response, functional acknowledgement, invoice flowing inside ICOE. It gives flexibility to customers to track their transactions and appropriate error information in-case of any failure. Based on resource based access control the tool gives flexibility to end user to perform different actions like view transactions, search based on different filter criteria and view and download actual message payload. Technology/Tools used SQL server 2014, SSIS, .net API, Angular JS. Responsibilities ETL solution to transform business transactions data stored in Biztalk tables. SQL azure tables, stored procedures, User defined functions. Performance tuning. Web API enhancements. EDUCATION The University of Manchester - UK 2007 SKILLS problem solving (Less than 1 year), project lifecycle (Less than 1 year), project manager (Less than 1 year), technical assistance. (Less than 1 year) ADDITIONAL INFORMATION Professional Skills Excellent analytical, problem solving, communication, knowledge transfer and interpersonalskills with ability to interact with individuals at all the levels Quick learner and maintains cordial relationship with project manager and team members andgood performer both in team and independent job environments Positive attitude towards superiors &amp; peers Supervised junior developers throughout project lifecycle and provided technical assistance.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, \"   ->>>>>>>> \", ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_PpCUktz5bT",
        "outputId": "bbe82127-18bd-4514-8fcb-5b50e0cf1e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Michael Smith    ->>>>>>>>  Name\n",
            "BI    ->>>>>>>>  Degree\n",
            "Manchester    ->>>>>>>>  Location\n",
            "indeed.com/r/falicent/140749dace5dc26f    ->>>>>>>>  Email Address\n",
            "10+    ->>>>>>>>  Years of Experience\n",
            "The University of Manchester    ->>>>>>>>  College Name\n",
            "2007    ->>>>>>>>  Graduation Year\n",
            "problem solving (Less than 1 year), project lifecycle (Less than 1 year), project manager (Less than 1 year), technical assistance. (Less than 1 year) ADDITIONAL INFORMATION Professional Skills Excellent analytical, problem solving, communication, knowledge transfer and interpersonalskills with ability to interact with individuals at all the levels Quick learner and maintains cordial relationship with project manager and team members andgood performer both in team and independent job environments Positive attitude towards superiors &amp; peers Supervised junior developers throughout project lifecycle and provided technical assistance.    ->>>>>>>>  Skills\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_-PXkWR0Bjb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBy8SL6MafzX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}